import os
import pandas as pd
import psycopg2
import numpy as np
import xgboost as xgb

from functools import partial
import optuna

from sklearn.metrics import mean_squared_error
import pickle

import boto3
from botocore.client import Config

import mlflow
from pycaret.regression import *
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import train_test_split

os.environ["MLFLOW_S3_ENDPOINT_URL"] = os.getenv("MLFLOW_S3_ENDPOINT_URL", "http://mlflow-artifact-store:9000")
os.environ["MLFLOW_TRACKING_URI"] = os.getenv("MLFLOW_TRACKING_URI", "http://mlflow-server:5000")
os.environ["AWS_ACCESS_KEY_ID"] = os.getenv("AWS_ACCESS_KEY_ID")
os.environ["AWS_SECRET_ACCESS_KEY"] = os.getenv("AWS_SECRET_ACCESS_KEY")


def save_scaler_to_minio(scaler, bucket_name, object_name):
    """
    í•™ìŠµëœ scaler ê°ì²´ë¥¼ pickleë¡œ ì €ì¥í•˜ê³ , MinIOì— ì—…ë¡œë“œí•˜ëŠ” í•¨ìˆ˜.
    bucket_name: ì—…ë¡œë“œí•  ë²„í‚· ì´ë¦„ (ì˜ˆ: "mlflow")
    object_name: ì €ì¥í•  íŒŒì¼ëª… (ì˜ˆ: "scaler.pkl")
    """
    # scaler ê°ì²´ë¥¼ íŒŒì¼ë¡œ ì €ì¥
    with open("scaler.pkl", "wb") as f:
        pickle.dump(scaler, f)

    # boto3 í´ë¼ì´ì–¸íŠ¸ ìƒì„± (MinIO)
    s3 = boto3.client(
        "s3",
        endpoint_url=os.getenv("MLFLOW_S3_ENDPOINT_URL"),
        aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
        aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
        config=Config(signature_version="s3v4"),
        region_name="us-east-1"
    )

    # íŒŒì¼ ì—…ë¡œë“œ
    s3.upload_file("scaler.pkl", bucket_name, object_name)
    print(f"Scaler ì €ì¥ ì™„ë£Œ: {bucket_name}/{object_name}")


class Preprocessor:
    """ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self):
        self.X_features = ["ì¸µ", "ë²•ì •ë™ì½”ë“œ", "ê±´ì¶•ë…„ë„", "ê±´ë¬¼ë©´ì _ã¡"]
        self.y_target = "ë¬¼ê±´ê¸ˆì•¡_ë§Œì›"
        self.scaler = RobustScaler()

    def fetch_data_from_db(self):
        """DBì—ì„œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°"""
        print("ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")

        query = "SELECT * FROM estate_details;"

        with psycopg2.connect(
                user=os.getenv("TRAIN_USER"),
                password=os.getenv("TRAIN_PASSWORD"),
                host=os.getenv("TRAIN_HOST"),
                database=os.getenv("TRAIN_DB"),
                port=5432
        ) as conn:
            with conn.cursor() as cursor:
                cursor.execute(query)
                records = cursor.fetchall()

                # âœ… DataFrame ë³€í™˜
                df = pd.DataFrame(records, columns=[desc[0] for desc in cursor.description])

        if df.empty:
            raise ValueError("ğŸš¨ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")

        print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ! {df.shape[0]}ê°œì˜ ë°ì´í„°")
        return df

    def preprocess_and_scale(self, df):
        """
        ë°ì´í„° ì „ì²˜ë¦¬ ë° ìŠ¤ì¼€ì¼ë§:
          - ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
          - í•„ìš”í•œ ì»¬ëŸ¼ ì„ íƒ
          - ê²°ì¸¡ì¹˜ ì²˜ë¦¬
          - íƒ€ê²Ÿ ë³€ìˆ˜ ë¡œê·¸ ë³€í™˜ (np.log1p)
          - ë…ë¦½ ë³€ìˆ˜ì— RobustScaler ì ìš©
          - ìŠ¤ì¼€ì¼ë§ëœ ë…ë¦½ ë³€ìˆ˜ì™€ ë¡œê·¸ ë³€í™˜ëœ íƒ€ê²Ÿì„ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ë°˜í™˜ (íƒ€ê²Ÿ ì»¬ëŸ¼ëª…: 'target')
        """
        print("ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ë° ìŠ¤ì¼€ì¼ë§ ì¤‘...")
        # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±° ë° í•„ìš”í•œ ì»¬ëŸ¼ ì„ íƒ
        df = df.drop(columns=["id", "estate_id"], errors="ignore")
        df = df[self.X_features + [self.y_target]]

        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        df.fillna(df.median(), inplace=True)

        # íƒ€ê²Ÿ ë³€ìˆ˜ ë¡œê·¸ ë³€í™˜ (np.log1p)
        df[self.y_target] = np.log1p(df[self.y_target])

        # ë…ë¦½ ë³€ìˆ˜ ìŠ¤ì¼€ì¼ë§ (RobustScaler)
        X_scaled = self.scaler.fit_transform(df[self.X_features])
        df_scaled = pd.DataFrame(X_scaled, columns=self.X_features)
        save_scaler_to_minio(self.scaler, bucket_name="mlflow", object_name="scaler.pkl")

        # ë¡œê·¸ ë³€í™˜ëœ íƒ€ê²Ÿ ë³€ìˆ˜ ì¶”ê°€ (ì»¬ëŸ¼ëª…: 'target')
        df_scaled["target"] = df[self.y_target].values
        print("âœ… ë°ì´í„° ì „ì²˜ë¦¬ ë° ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ!")
        return df_scaled


class Automation:
    """PyCaret AutoML ë° DB ì €ì¥ í´ë˜ìŠ¤"""

    def __init__(self, preprocessor):
        self.preprocessor = preprocessor
        self.results_df = None
        # DB ì—°ê²° ì„¤ì • (í•„ìš”í•œ ê²½ìš° ì—¬ê¸°ì„œ ë¯¸ë¦¬ ì—°ê²°í•˜ê±°ë‚˜, ì €ì¥ ì‹œì ì— ìƒì„±)
        self.conn = psycopg2.connect(
            user=os.getenv("TRAIN_USER"),
            password=os.getenv("TRAIN_PASSWORD"),
            host=os.getenv("TRAIN_HOST"),
            database=os.getenv("TRAIN_DB")
        )
        self.cursor = self.conn.cursor()

    def train_pycaret(self):
        """PyCaret AutoML ì‹¤í–‰ ë° ê²°ê³¼ DB ì €ì¥"""
        df = self.preprocessor.fetch_data_from_db()
        df_scaled = self.preprocessor.preprocess_and_scale(df)

        mlflow.set_experiment("pycaret_automl")
        print("ğŸ“Š PyCaret AutoML ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")

        with mlflow.start_run():
            setup(data=df_scaled, target="target", log_experiment=True, experiment_name="pycaret_automl", verbose=False)
            best_model = compare_models()
            self.results_df = pull()

            unique_model_names = ", ".join(self.results_df["Model"].unique())
            mlflow.log_param("model_name", unique_model_names)

            for _, row in self.results_df.iterrows():
                mlflow.log_metric("mae", row["MAE"])
                mlflow.log_metric("r2", row["R2"])
                mlflow.log_metric("rmse", row["RMSE"])

        # í•™ìŠµ ê²°ê³¼ DBì— ì €ì¥
        self.save_results_to_db()

        # DB ì—°ê²° ì¢…ë£Œ (ì›í•œë‹¤ë©´ ì—¬ê¸°ì„œ ì¢…ë£Œí•˜ê±°ë‚˜, ë³„ë„ë¡œ ê´€ë¦¬)
        self.cursor.close()
        self.conn.close()

        return self.results_df

    def save_results_to_db(self):
        """PyCaret í‰ê°€ ê²°ê³¼ë¥¼ DBì— ì €ì¥"""
        if self.results_df is None:
            raise ValueError("ğŸš¨ ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        for _, row in self.results_df.iterrows():
            self.cursor.execute(
                "INSERT INTO model_results (model_name, mae, r2, rmse) VALUES (%s, %s, %s, %s);",
                (row["Model"], row["MAE"], row["R2"], row["RMSE"])
            )
        self.conn.commit()
        print("âœ… DBì— ê²°ê³¼ ì €ì¥ ì™„ë£Œ!")


class EstatePredict:
    """XGBoost ëª¨ë¸ (Optuna ìƒëµí•œ ì˜ˆì‹œ)"""

    def __init__(self, preprocessor):
        self.preprocessor = preprocessor
        self.best_params = {
            "n_estimators": 100,
            "max_depth": 6,
            "learning_rate": 0.1,
            "subsample": 1.0,
            "colsample_bytree": 1.0
        }
        self.model = xgb.XGBRegressor(**self.best_params, random_state=42)

    def fit(self):
        """XGBoost ëª¨ë¸ í•™ìŠµ ë° MLflow ë¡œê¹…"""
        print("ğŸ“Š XGBoost ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        print("í›ˆë ¨ì¤‘...")  # ì§„í–‰ ë©”ì‹œì§€ ì¶œë ¥

        # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ + ìŠ¤ì¼€ì¼ë§ (ë‹¨ì¼ DataFrame ë°˜í™˜)
        df = self.preprocessor.fetch_data_from_db()
        df_scaled = self.preprocessor.preprocess_and_scale(df)

        # train/test split (ì˜ˆ: 80:20 ë¹„ìœ¨)
        X = df_scaled[self.preprocessor.X_features]
        y = df_scaled["target"]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # MLflow ì‹¤í—˜ ì„¤ì •
        mlflow.set_experiment("xgboost_optuna")
        with mlflow.start_run():
            mlflow.log_params(self.best_params)

            # ëª¨ë¸ í•™ìŠµ
            self.model.fit(
                X_train, y_train,
                eval_set=[(X_test, y_test)],
                verbose=True
            )

            # ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
            y_pred = self.model.predict(X_test)
            mae = np.mean(np.abs(y_test - y_pred))
            rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))
            r2 = 1 - (np.sum((y_test - y_pred) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2))

            mlflow.log_metric("mae", mae)
            mlflow.log_metric("rmse", rmse)
            mlflow.log_metric("r2", r2)
            mlflow.xgboost.log_model(self.model, artifact_path="xgboost_model")

            print(f"âœ… MLflowì— ëª¨ë¸ ì €ì¥ ì™„ë£Œ! (MAE: {mae:.4f}, RMSE: {rmse:.4f}, RÂ²: {r2:.4f})")





# class EstatePredict:
#     """XGBoost ëª¨ë¸ (Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í¬í•¨)"""
#
#     def __init__(self, preprocessor):
#         self.preprocessor = preprocessor
#         self.best_params = None
#         self.model = None
#
#     def fit(self):
#         """Optunaë¥¼ í†µí•œ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ í›„ ëª¨ë¸ í•™ìŠµ ë° MLflow ë¡œê¹…"""
#         print("ğŸ“Š XGBoost ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
#         print("í›ˆë ¨ì¤‘...")
#
#         # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ + ìŠ¤ì¼€ì¼ë§ (ë‹¨ì¼ DataFrame ë°˜í™˜)
#         df = self.preprocessor.fetch_data_from_db()
#         df_scaled = self.preprocessor.preprocess_and_scale(df)
#
#         # train/test split (ì˜ˆ: 80:20 ë¹„ìœ¨)
#         X = df_scaled[self.preprocessor.X_features]
#         y = df_scaled["target"]
#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
#
#         # objective í•¨ìˆ˜ ì •ì˜ (ë³„ë„ì˜ í•¨ìˆ˜ë¡œ ë¶„ë¦¬ ê°€ëŠ¥)
#         def objective(trial, X_train, y_train, X_test, y_test):
#             params = {
#                 "n_estimators": trial.suggest_int("n_estimators", 50, 500),
#                 "max_depth": trial.suggest_int("max_depth", 3, 20),
#                 "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
#                 "subsample": trial.suggest_float("subsample", 0.5, 1.0),
#                 "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
#                 "random_state": 42,
#                 "n_jobs": -1
#             }
#             model = xgb.XGBRegressor(**params)
#             model.fit(X_train, y_train, eval_set=[(X_test, y_test)])
#             y_pred = model.predict(X_test)
#             rmse = np.sqrt(mean_squared_error(y_test, y_pred))
#             return rmse
#
#         # partialì„ ì‚¬ìš©í•˜ì—¬ objective í•¨ìˆ˜ì— ë°ì´í„° ì „ë‹¬
#
#         objective_func = partial(objective, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)
#         study = optuna.create_study(direction="minimize")
#         study.optimize(objective_func, n_trials=50)
#         print("Optuna ìµœì  íŒŒë¼ë¯¸í„°:", study.best_params)
#         self.best_params = study.best_params
#
#         # ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
#         self.model = xgb.XGBRegressor(**self.best_params, random_state=42, n_jobs=-1)
#         mlflow.set_experiment("xgboost_optuna")
#         with mlflow.start_run():
#             mlflow.log_params(self.best_params)
#             self.model.fit(
#                 X_train, y_train,
#                 eval_set=[(X_test, y_test)],
#                 verbose=True,
#             )
#
#             y_pred = self.model.predict(X_test)
#             mae = np.mean(np.abs(y_test - y_pred))
#             rmse = np.sqrt(mean_squared_error(y_test, y_pred))
#             r2 = 1 - (np.sum((y_test - y_pred) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2))
#
#             mlflow.log_metric("mae", mae)
#             mlflow.log_metric("rmse", rmse)
#             mlflow.log_metric("r2", r2)
#             mlflow.xgboost.log_model(self.model, artifact_path="xgboost_model_optuna")
#
#             print(f"âœ… MLflowì— ëª¨ë¸ ì €ì¥ ì™„ë£Œ! (MAE: {mae:.4f}, RMSE: {rmse:.4f}, RÂ²: {r2:.4f})")